# Titanic Challenge
The [Kaggle challenge](https://www.kaggle.com/competitions/titanic) to train a model to predict survivors in the Titanic is an ongoing challenge. I participated and submitted various models. In the Jupyter Notebook you can find detailed strategies and models that I used.

the best model I used gave me a 0.80622 score, ranking me tied in the 330th spot out of 14468 total people who have attempted this problem, putting me in the top 2-3% of the overall score 


# The Mushroom Challenge
The [Kaggle challenge](https://www.kaggle.com/competitions/playground-series-s4e8) to train a model to recognize poisonous mushrooms is a challenge on Kaggle. In the Jupyter Notebook, I used an XGBoost model to gain a 98.4% accuracy, I decided to keep the NaN values on the training and testing data as there was a significant amount and, especially, very similar percentages between the training and testing data

# The MNIST Dataset Challenge
The [MNIST challenge](https://www.kaggle.com/competitions/digit-recognizer) is one of the more famous datasets to use when learning about computer vision, especially using CNNs. In the Jupyter Notebook, I used a CNN to gain a 0.99167 score. This dataset is very good for training and understanding the different components of a CNN and how to change/ adapt them to get better results
